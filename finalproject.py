# -*- coding: utf-8 -*-
"""FinalProject

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fU7P0gz6vWw5sWdW3qkoAn3lJyjloy_E
"""

import pandas as pd

sample =pd.read_csv('supermarket_sales.csv')
print(sample)

"""'''(Q6 clo2)Create a script to develop a Python function for descriptive statistics. The input for the function should be the sample and the field to perform the descriptive statistics.'''"""

from matplotlib.colors import Normalize
def Desc_stat(sample,var):
  mean=sample[var].mean()
  median=sample[var].median()
  mode=sample[var].mode()
  minimum=sample[var].min()
  maximum=sample[var].max()
  range=sample[var].max() - sample[var].min()
  standard_deviation=sample[var].std()
  variance=sample[var].var()
  skew=sample[var].skew()
  kurtosis=sample[var].kurt()
  count=sample[var].count()
  Quart = [
            sample[var].quantile(0),
            sample[var].quantile(0.25),
            sample[var].quantile(0.50),
            sample[var].quantile(0.75),
            sample[var].quantile(1),
            sample[var].quantile(0.75) - sample[var].quantile(0.25)
          ]

  print('the mean is ', mean)
  print('the median is', median)
  print('the mode is',mode)
  print('the minimum is ',minimum)
  print('The maximum is',maximum)
  print('The range is',range)
  print('The standard deviation is',standard_deviation)
  print('The variance is',variance)
  print('The skewness is', skew)
  print('The kurtosis is', kurtosis)
  print('The count is', count)
  print('The Quart and the iqr is ',Quart)

# Call the function with your data
Desc_stat(sample, 'Total')

""""(Q7 clo2)Create a program to random sampling of size 150 and find the descriptive statistics for the dependent variable from the sample [Apply the descriptive function which you created]."

"""

data = pd.read_csv('supermarket_sales.csv')

# Generate a random sample of the data
sample_size = 150
sample1 = data.sample(n=sample_size, replace=False, random_state=42)

print(sample1)





from matplotlib.colors import Normalize
def Desc_stat(sample1,var):
  mean=sample1[var].mean()
  median=sample1[var].median()
  mode=sample1[var].mode()
  minimum=sample1[var].min()
  maximum=sample1[var].max()
  range=sample1[var].max() - sample1[var].min()
  standard_deviation=sample1[var].std()
  variance=sample1[var].var()
  skew=sample1[var].skew()
  kurtosis=sample1[var].kurt()
  count=sample1[var].count()
  Quart = [
            sample1[var].quantile(0),
            sample1[var].quantile(0.25),
            sample1[var].quantile(0.50),
            sample1[var].quantile(0.75),
            sample1[var].quantile(1),
            sample1[var].quantile(0.75) - sample1[var].quantile(0.25)
          ]

  print('the mean is ', mean)
  print('the median is', median)
  print('the mode is',mode)
  print('the minimum is ',minimum)
  print('The maximum is',maximum)
  print('The range is',range)
  print('The standard deviation is',standard_deviation)
  print('The variance is',variance)
  print('The skewness is', skew)
  print('The kurtosis is', kurtosis)
  print('The count is', count)
  print('The Quart and the iqr is ',Quart)

# Call the function with your data
Desc_stat(sample1, 'Total')

""""(Q8 clo2)Create a script for systematic sampling by giving certain conditions and finding the desc stat for the dependent variable from the sample [Apply the descriptive function which you created]."
"""

# Q 8 clo2
import pandas as pd
sample = pd.read_csv('supermarket_sales.csv')


condition = (sample['City'] == 'Yangon') & (sample['Payment'] == 'Ewallet')
filtered_df = sample[condition].reset_index(drop=True)

def systematic_sampling(data, sample_size):
    if sample_size == 0 or len(data) == 0:
        return pd.DataFrame()
    step = max(len(data) // sample_size, 1)
    return data.iloc[::step][:sample_size]

# Apply systematic sampling on the filtered data
sample = systematic_sampling(filtered_df, sample_size=50)  # sample of 50

# Descriptive function for a dependent variable
def descriptive_stats(series):
    return {
        "count": series.count(),
        "mean": series.mean(),
        "std_dev": series.std(),
        "min": series.min(),
        "25%": series.quantile(0.25),
        "median": series.median(),
        "75%": series.quantile(0.75),
        "max": series.max()
    }

# Example: Descriptive stats for 'Total' column
desc_stats = descriptive_stats(sample['Total'])
print("Descriptive Statistics for 'Total':")
for k, v in desc_stats.items():
    print(f"{k}: {v:.2f}")

""""(Q 9 clo2)Create a detailed descriptive statistics report about the dependent variable of the chosen dataset"
"""

# Q 9 clo2
import pandas as pd

# Define the descriptive statistics function
def descriptive_stats(series):
    return {
        "Count": series.count(),
        "Mean": series.mean(),
        "Std Dev": series.std(),
        "Min": series.min(),
        "25% (Q1)": series.quantile(0.25),
        "Median (Q2)": series.median(),
        "75% (Q3)": series.quantile(0.75),
        "Max": series.max()
    }

# Dependent variables
total_stats = descriptive_stats(sample["Total"])
gross_income_stats = descriptive_stats(sample["gross income"])
rating_stats = descriptive_stats(sample["Rating"])

# Combine into a DataFrame for easy viewing
report = pd.DataFrame({
    "Total": total_stats,
    "Gross Income": gross_income_stats,
    "Rating": rating_stats
})

print("ðŸ“Š Descriptive Statistics Report:\n")
print(report.round(2))

""""(Q10 clo2)Visualize the dependent variable by the Graph/Chart of the following using Python Program: a. Scatter plot b. Box Plot c. Histogram d. Heat Map Hint: Use Matplot or Ski-learn library"
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv("supermarket_sales.csv")

# Set the style
sns.set(style="whitegrid")

# Create the plots
fig, axs = plt.subplots(2, 2, figsize=(16, 12))

# a. Scatter Plot - Quantity vs Total
sns.scatterplot(data=df, x="Quantity", y="Total", ax=axs[0, 0], hue="Gender")
axs[0, 0].set_title("Scatter Plot of Quantity vs Total")

# b. Box Plot - Total grouped by Payment type
sns.boxplot(data=df, x="Payment", y="Total", ax=axs[0, 1])
axs[0, 1].set_title("Box Plot of Total by Payment Type")

# c. Histogram - Distribution of Total
sns.histplot(data=df, x="Total", bins=30, kde=True, ax=axs[1, 0], color='orange')
axs[1, 0].set_title("Histogram of Total")

# d. Heatmap - Correlation heatmap of numeric variables
corr = df.select_dtypes(include='number').corr()
sns.heatmap(corr, annot=True, cmap="coolwarm", ax=axs[1, 1])
axs[1, 1].set_title("Heatmap of Numeric Correlations")

# Adjust layout
plt.tight_layout()
plt.show()

""""(Q11 clo2)Perform the hypothesis test to find the correlation (Pearson and Spearman for numerical variable and chi-square test for categorical variable) between the independent variable and the dependent variable.
Note: If you have more than one independent variable, then chose any one of the independent variables

"
"""

# Q 11 clo2
import pandas as pd
from scipy.stats import pearsonr, spearmanr, chi2_contingency

sample = pd.read_csv('supermarket_sales.csv')

x = sample['Unit price']
y = sample['Total']

# Pearson Correlation
pearson_corr, pearson_p = pearsonr(x, y)

# Spearman Correlation
spearman_corr, spearman_p = spearmanr(x, y)

print("\nCorrelation Between Unit Price & Total:")
print(f"Pearson Correlation: {pearson_corr:.2f} (p-value: {pearson_p:.4f})")
print(f"Spearman Correlation: {spearman_corr:.2f} (p-value: {spearman_p:.4f})")

if pearson_p < 0.05:
    print(" Pearson correlation is statistically significant.")
else:
    print(" Pearson correlation is NOT statistically significant.")

if spearman_p < 0.05:
    print("Spearman correlation is statistically significant.")
else:
    print(" Spearman correlation is NOT statistically significant.")

# Create a contingency table for Gender vs Customer type
contingency_table = pd.crosstab(sample['Gender'], sample['Customer type'])

# Apply Chi-Square Test
chi2, p_chi, dof, expected = chi2_contingency(contingency_table)

print("\nChi-Square Test Between Gender and Customer Type:")
print(f"Chi-Square Statistic: {chi2:.2f}")
print(f"p-value: {p_chi:.4f}")

if p_chi < 0.05:
    print("There is a statistically significant association between Gender and Customer Type.")
else:
    print("No statistically significant association between Gender and Customer Type.")

""""(Q12 clo2)Assess the performance of the dependent variable to know whether the sample is representative of the normal population by a one-sample t-test."
"""

# 12 clo2
import scipy
scipy.stats.ttest_1samp(sample['Rating'], 75)

""""(Q13 clo3)13.	Build, Train, Develop and Evaluate using Simple Regression for chosen dataset."
"""

# clo3 part 1  Importing the libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# importing the data set

dataset = pd.read_csv('supermarket_sales.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
print(X)
print(y)

# fillthe missing data with the mean from the unit price till the total
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X[:, 7:10])
X[:, 7:10] = imputer.transform(X[:, 7:10])
print(X)

# independent variables
X = dataset[['Unit price']]
# Dependent variable
y = dataset['Total']


#Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
#the test size is 20% so it will take 80% traing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)
print(X_train)
print(X_test)
print(y_train)
print(y_test)


#Training the Simple Linear Regression model on the Training set
from sklearn.linear_model import LinearRegression  # linear regression
regressor = LinearRegression()
regressor.fit(X_train, y_train)


# Predicting the Test set results
y_pred = regressor.predict(X_test)
print(X_test)
print(y_pred)


#Predicting the Specific Value
#find the total(dependent variable) for Unit price, Quantity,Tax 5%(ind var)=1.3389109746091776
predicted_total = regressor.predict([[99.96]])
print(predicted_total)


#Visualising the Training set results we used the x_train iloc to make them in the same size
plt.scatter(X_train.iloc[:, 0], y_train, color = 'pink')
plt.plot(X_train.iloc[:, 0], regressor.predict(X_train), color = 'purple')
plt.title('Unit price vs total (Training set)')
plt.xlabel('Unit price')
plt.ylabel('total')
plt.show()


#Visualising the Test set results
plt.scatter(X_test.iloc[:, 0], y_test, color =  'green')
plt.plot(X_train.iloc[:, 0], regressor.predict(X_train), color = 'orange')
plt.title('Unit price vs total (Training set)')
plt.xlabel('Unit price')
plt.ylabel('total')
plt.show()


#Getting the final linear regression equation
print(regressor.coef_)
print(regressor.intercept_)

#Total = 5.9 * Unit price - 1.3389

""""(Q14 clo3)Develop a script to forecast the value of the dependent variable from all the relevant independent variables using Multiple Linear Regression"
"""

# clo3 part 2

#  Importing the libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# importing the data set

dataset = pd.read_csv('supermarket_sales.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
print(X)
print(y)

# fillthe missing data with the mean from the unit price till the total
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X[:, 7:10])
X[:, 7:10] = imputer.transform(X[:, 7:10])
print(X)

# independent variables
X = dataset[['Unit price', 'Quantity','Tax 5%']]
# Dependent variable
y = dataset['Total']


#Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
#the test size is 30% so it will take 70% traing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)


#Training the Multiple Linear Regression model on the Training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)



# Predicting the Test set results
y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.values.reshape(len(y_test),1)),1))



#Predicting the Specific Value
#find the total(dependent variable) for Unit price, Quantity,Tax 5%(ind var)=60.82
predicted_total = regressor.predict([[14.48, 4, 2.896]])
print(predicted_total)



#Getting the Final Multiple Linear Regression Equation
print(regressor.coef_)
print(regressor.intercept_)

#Total = (-4.60e-15 * Unit price) + (-2.49e-14 * Quantity) +
 #(2.10e+01 * Tax 5%) + 2.2737367544323206e-13

""""(Q15 CLO3)	Predict the value of the dependent variable from the different classifier such as Logistic Regression, KNN, NaÃ¯ve-Bayes and Decision Tree."
"""

#Logistic Regression:
#Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score # Import necessary metrics

# import the data set
dataset = pd.read_csv('supermarket_sales.csv')
X = dataset[['Total','gross income','Rating','Unit price','Quantity','Tax 5%']]
y = dataset['Customer type']


#split into the train and test
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

print (x_train)

print (x_test)


#feature scaling
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)


#training the logisttic
from sklearn.linear_model import LogisticRegression
classifier=LogisticRegression(random_state=0)
classifier.fit(x_train,y_train)


# making the confusion matrix
y_pred = classifier.predict(x_test) # Use x_test instead of X_test
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", cm)
print("Accuracy Score:", round(accuracy_score(y_test, y_pred), 4))
print("Total Errors:", cm[0][1] + cm[1][0])

#evaluate
#from sklearn.metrics import accuracy_score # Already imported above

#from sklearn.metrics import accuracy_score # Already imported above
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)


prediction = classifier.predict(sc.transform([[548.9715,26.1415,9.1,74.69,7,26.1415]]))
print("\nPrediction =", prediction)

""""(Q 16 clo3)Evaluate the performance of each model using confusion matrix and accuracy and identify the best fit classifier for the chosen dataset."
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier # Import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB  # Import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import SMOTE

# ... (rest of your code) ...

dataset = pd.read_csv('supermarket_sales.csv')
X = dataset[['Total','gross income','Rating','Unit price','Quantity','Tax 5%']]
y = dataset['Customer type']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=1)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Models to train
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),

}

# Evaluate each model
evaluation_metrics = []
best_model = {"Model": None, "Accuracy": 0}  # Initialize best_model

for name, model in models.items():
    model.fit(X_train_resampled, y_train_resampled)
    y_pred = model.predict(X_test)

    # Calculate accuracy confusion_matrix and error
    accuracy = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    errors = cm[0][1] + cm[1][0]
    # Setting pos_label to 'Member' to align with the actual labels in your data.
    precision = precision_score(y_test, y_pred, pos_label='Member', zero_division=0)
    recall = recall_score(y_test, y_pred, pos_label='Member')
    f1 = f1_score(y_test, y_pred, pos_label='Member')

    # Store the metrics
    evaluation_metrics.append({
        "Model": name,
        "Accuracy": accuracy,
        "Errors" : errors,
        "confusion_matrix": cm
   })

    # Update best_model if current model has higher accuracy
    if accuracy > best_model["Accuracy"]:
        best_model["Model"] = name
        best_model["Accuracy"] = accuracy


# Convert metrics to DataFrame
metrics_df = pd.DataFrame(evaluation_metrics)

# Print evaluation metrics for all models
print("\nEvaluation Metrics for All Models:")
print(metrics_df)

# Identify the best-fit model based on Accuracy
print("\nBest-Fit Model:")
print(f"Model: {best_model['Model']}")  # Now best_model is defined

"""(Q17 clo3)Predict the dependent variable by using best-fit classifier"""

# Q 17 clo3
# Step 1: Importing the libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Step 2: Importing the dataset

df = pd.read_csv("supermarket_sales.csv")

# Convert 'Rating' to categorical class labels
def classify_rating(rating):
    if rating < 5:
        return 'Low'
    elif rating < 8:
        return 'Medium'
    else:
        return 'High'

df['Rating_Class'] = df['Rating'].apply(classify_rating)

# Encode categorical variables
label_cols = ['Gender', 'Payment', 'Product line', 'Customer type', 'City']
for col in label_cols:
    df[col] = LabelEncoder().fit_transform(df[col])

# Step 3: Splitting the dataset into the Training set and Test set
X = df[['Unit price', 'Quantity', 'Total', 'gross income', 'Gender', 'Payment', 'Product line']]
y = df['Rating_Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 5: Training the Decision Tree Classification model on the Training set
classifier = DecisionTreeClassifier(random_state=42)
classifier.fit(X_train, y_train)

# Step 6: Making the Confusion Matrix and evaluating the model
y_pred = classifier.predict(X_test)

print("ðŸŽ¯ Decision Tree Classifier Results:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

""""(Q18 clo3)Perform the cluster analysis such as K-means and Horizontal for any field from the chosen dataset."
"""

#Q 18 clo3 (part 1)
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt


dataset = pd.read_csv('supermarket_sales.csv')
#2 independent

X = dataset[['Unit price', 'Quantity','Tax 5%']].values

wcss_list = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss_list.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss_list)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.title('Elbow Method for Determining Optimal k')
plt.show()

kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)
y_kmeans = kmeans.fit_predict(X)

print(y_kmeans)

"""# Q 18 clo3(part 2)"""

# Q 18 clo3(part 2)

# Step 1: Importing the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.preprocessing import StandardScaler

# Step 2: Importing the dataset

X = sample[['Total', 'gross income', 'Rating']]

# Standardizing the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Using the dendrogram to find the optimal number of clusters
plt.figure(figsize=(10, 5))
dendrogram(linkage(X_scaled, method='ward'))
plt.title("Dendrogram")
plt.xlabel("Observations")
plt.ylabel("Euclidean Distances")
plt.show()

# Step 4: Training the Hierarchical Clustering model on the dataset
linked = linkage(X_scaled, method='ward')

clusters = fcluster(linked, 3, criterion='maxclust')
sample['Hierarchical_Cluster'] = clusters

# Step 5: Visualising the clusters
plt.figure(figsize=(8, 5))
sns.scatterplot(x='Total', y='Rating', hue='Hierarchical_Cluster', data=sample, palette='Set1')
plt.title("Hierarchical Clustering (Total vs Rating)")
plt.xlabel("Total")
plt.ylabel("Rating")
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

""""(Q19 clo2)Explain the strategy for improving the system after viewing the cluster diagram."
"""

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt


dataset = pd.read_csv('supermarket_sales.csv')

X = dataset[['Unit price', 'Quantity']].values

wcss_list = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss_list.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss_list)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.title('Elbow Method for Determining Optimal k')
plt.show()

kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
y_kmeans = kmeans.fit_predict(X)

print(y_kmeans)